#mx站点各类url采集工具

###代码文件说明
1. main.py:入口函数文件
2. common.py:程序配置文件,全局变量全部放在Config类中,类属性形式调用(非实例属性)
3. get_max_sum.py:通过动态规划算法计算一串数字的最长连续子序列和
4. spider.py:2&3级页面采集算法

###采集流程说明
####二级url:

1. 找到页面所有a链接并记录所有a链接的textContent长度
2. 从前往后依次比较相邻两个a链接文本长度,相同则记录1,否则记录为-1,然后存放到数组中
3. 利用动态规划算法求解该数组的最长连续子序列和(最长连续子序列意味着整个数组中最集中的相同字数区域),然后求出对应的在数组中索引的位置
4. 遍历a链接集合,依次找到包含图片|电影|快播|视频|图片关键字,利用此类关键字作为类型区分(图片小说视频)
5. 从上面找到的每一个链接后面的链接,至此找到最终的二级页面url

####三级url:
首先有一个前提条件,即同一个网站下同一个类型的url具有比较高的相似度(有些只是一个id不同而已,大部分url path是相同的),这个相似度的benchmark可以在common.py文件中配置

1. 遍历页面所有a连接
2. 计算当前链接和上一个链接的href的编辑距离(一种利用动态规划算法计算两个字符串差异度),剩下的思路与2级页面相同,编辑距离差小于3存成1,否则存成-1
3. 通过求解最长连续序列和算法找出对应的链接范围,然后从其中取出来一个即是3级url


###相比传统方法优势
因目前普通的像request,httplib2,urllib2等采集工具遇到以下几个问题的时候无法解决,故通过selenium解决

1. 通过js实现跳转 
2. 部分页面内容是通过js动态生成 
3. 部分页面编码类型(charset)标识错误,导致出现乱码(之前被繁体字搞崩溃了,最后将有繁体字的页面由用gbk代替gb2312 decode即可) 

###相比传统方法劣势
1. selenium使用时候cpu负载高,几乎100%
2. 效率差,目前采用多进程(程序本身多进程,每个进程对应的一个浏览器进程)采集方式采集,但是效率依然低下,因为浏览器要渲染页面解析js下载图片等
3. 由于获取三级url的时候需要计算相邻两个链接的href的编辑距离,故如果站点每个href都是完全不同的命名规则那么将寻找失败(该类站点目前还没看到)

###执行效率

1. 目前的执行效率比较低,30个网站,总计210个(每个网站7种类型)页面耗时900秒,具体和网速,电脑等相关
2. 每个网站耗时30秒,每个页面耗时4.5秒

###执行准确率

目前执行准确率,随机测试30个网站结果如下

1. 2级图片:有效28个
2. 2级小说:有效28个
3. 2级视频:有效26个
4. 3级图片:有效24个
5. 3级小说:有效24个
6. 3级视频:有效22个

总的来说,二级页面准确率90%左右,三级页面准确率在80%左右

###待优化问题

1. 通过selenium调用PhantomJS,但是目前PhantomJS在某些页面上出现乱码,没有很好的解决办法
2. phantomjs是后台的浏览器进程,相比原生浏览器效率高很多,后续可以使用起来
