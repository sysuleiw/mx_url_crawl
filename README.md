#mx站点各类url采集工具

###代码文件说明
1. main.py:入口函数
2. common.py:配置文件
3. get_max_sum.py:利用动态规划算法计算一串数字的最长公共子序列和
4. spider.py:2&3级页面采集算法

###采集流程说明
####二级url:
1. 找到页面所有a链接并记录所有a链接的textContent长度
2. 长度相同则记录1,否则记录为-1,支持如果有n个a,则有n-1个1和-1组成的序列,然后存放到数组中
3. 利用求最长公共子序列和的方法求解范围
4. 根据范围找到对应的a链接范围
5. 找到图片|电影|快播|视频|图片关键字,利用此类关键字作为类型区分
6. 从上面找到的每一个链接后面的链接,至此找到最终的二级页面url

####三级url:
1. 遍历页面所有a连接
2. 计算当前链接和上一个链接的href的编辑距离(一种利用动态规划算法计算两个字符串差异度)差,思路同上面一个步骤的相同,编辑距离差小于3存成1,否则存成-1
3. 利用最长序列和算法找出对应的链接范围,然后从其中取出来一个即可


###所需工具
因目前普通的像request,httplib2,urllib2等采集工具遇到以下几个问题的时候无法解决,故通过selenium解决,具体可以了解selenium
1. 通过js跳转的 
2. 部分页面内容是通过js动态生成  

###目前隐藏问题
1. selenium调用浏览器的时候cpu负载高,几乎100%
2. 效率差,目前采用多进程爬取方式,但是效率依然低下,因为浏览器要渲染页面解析js下载图片等

###待优化问题

1. 通过selenium调用PhantomJS,但是目前PhantomJS在某些页面上出现乱码,没有很好的解决办法
2. phantomjs是后台的浏览器进程,相比原生浏览器效率高很多,后续可以使用起来

